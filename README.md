# League of Legends Position Prediction Model
## Framing the Problem
For this project, we will continue to use the League of Legends dataset. The prediction problem that we will be answering for this project is given a player's post-game data, predict their position. To answer this question we are going to be using multiclass classification. Our response variable is going to be the ‘position’ as this tells us if our model predicts correctly. We chose this because we are trying to do a classification problem and we thought it would be interesting to see if there were certain characteristics of a player who plays a certain position and what their stats would be like. The classes that we will be predicting are: ‘top’, ‘jng’, ‘mid’, ‘bot’, and ‘sup’. For this model, we will be evaluating it by using accuracy. Accuracy best describes our data because it would be the most useful in explaining how close we were to the true position. It would tell us exactly how many of those positions we correctly predict. Using the accuracy would be better than using F1-score, precision, or recall because it better encapsulates and represents the data as a whole. The other three error methods we may have chosen are better for data that is binary in its classification as they use false negatives, false positives, true negatives, and true positives. However, in our case, since we're doing a multiclassification problem, False Positive and many other elements that are used in recall, precision, and F1 do not make very much sense here. Since we are given post-game data, we are free to use all the columns to help us with our prediction, except for using the position column since that's what we're trying to predict, since we know all that statistics up front before we make some prediction. 

## Baseline Model
We created a baseline model to predict the ‘position’ in order to serve as a benchmark that our final model could use to compare its performance against. In our baseline model, we utilized a decision tree classifier with a max depth of 4 to predict the position. For this model, we used two features to predict the ‘position’ on. 
The first feature we used in the model was the amount of gold earned per minute for each player (‘earned gpm’). The ‘earned gpm’ was first standardized to using standard scaler and was also imputed using the median of the standard scaler to replace the missing values with. This feature is a continuous quantitative feature as it can be within a large range of numbers without a definite upper limit and it does not contain small ‘buckets’ like discrete quantitative data does. The second feature we used in the model was the amount of damage inflicted to enemies as a proportion (‘damageshare’). The ‘damageshare’ went through a similar process as the ‘earned gpm’ as it was standardized using standard scaler and then imputed using the missing values with the median of the standard scaler. Like ‘earned gpm’, this feature is also a continuous quantitative feature as it can be within a large range of numbers without a definite upper limit and it does not contain small ‘buckets’ like discrete quantitative data does. So totally, we used two quantitative features for our baseline model. 
The performance of our baseline model was decently “good” as it was successfully able to predict the players’ position about 52.1% of the time. This is “good” because of the amount of ‘position’s there are in our data (5) meaning that if it was purely randomly guessing, the model would be expected to have an accuracy of about 20%. Our model is better than ‘purely guessing’ by a factor of more than 2.5x after implementing the baseline model described above to use the two features ‘earned gpm’ and ‘damageshare’.

## Final Model
On top of the features what we used in our Baseline model (earned gpm (quantative), damageshare (quantitative)), we also included the features of gameid (categorical/nominal), side (categorical/nominal), assists (quantitative), monsterkills (quantitative), gamelength (quantitative), champion (categorical/nominal), and damagemitigatedperminute (quantitative). There are a few reasons why we chose the features we stated above. First reason is that we knew a little bit about the typical characteristics of what certain positions had. For example, for monsterkills, we would expect the jng position to have the most monsterkills since they would be in the jungle killing off monsters for a greater amount of time of the game compared to other players with the other positions. We used a FunctionTransformer to tell us which for every game (which is why we need gameid), which player on each side (which is why we need the side feature) got the most monsterkills per minute. This brings up to why we included gamelength because in order to calculate the monsterkills per minute, we needed a feature that would tell us the duration of the game, which is what gamelength would do. Next, we also thought of using the assists feature since we noticed that players who played the sup position often had the highest assists. What this means is that we would expect that the player with the most assists in their game would be the player who played the sup position. So we used a FunctionTransformer which would tell us that in each game, was that player the one with the most assists on their side. Both of these features helped us a lot in identifying sup and jng positions. Afterwards, it wasn't super easy to identify features that pertained pointed heavily towards one group. But we did notice that the damagemitigatedperminute, when groupedby position, seemed to be binned in certain areas, implying that there was kind of a hierarchy or ordering. Some positions had higher damagemitigatedperminute, so we decided to impute and scale that feature, alongside earned gpm and damageshare, which we also chose for similar reasons. Lastly, we decided to OneHotEncode the champion feature. We noticed that champions are played by certain positions more than others. Since there are so many, we wanted to OneHotEncode it to just tell us if that was the champion the player used or not. This would help us a least a little bit since again, we believed that certain champions were more likely to be played by certain positions than others. 

We decided to use a Pipeline that used a DecisionTreeClassifier as our final model. We wanted to use a DecisionTreeClassifier for a few reasons. One is that this is a classification problem, so using some kind of LinearRegression model wouldn't make very much sense to use in comparison to a DecisionTreeClassifier. Two, this is actually quite similar to how we as humans make decisions, because when are presented with some information, we try to dissect it and that will push us in one direction or the other, which is kind of like traveling down the left child or right child. It just made the most sense in our heads, so that's why we chose this model. However, there may be other models that do something similar but this is what we decided to use since out of all the models we learned about, this one performs classiication the way we want it to. 

The hyperparameters we used were max_depth, which puts a bound to the depth of the tree, meaning that the depth can't be any more than max_depth, as well as min_samples_split, which is the minimum number of samples needed in order for one node to split into two. We wanted to tune these hyperparameters because of the fact that they do indeed change what the DecisionTreeClassifier will look like. Some combination of hyperparameters may even limit even the positions we can output since there are 5 outputs and a max_depth of 2 would only less us return a maximum of 4 types of positions. To find the best combination of hyperparameters, we decided to use GridSearchCV to go through all the combinations of the hyperparameters. In this case, we were going with the Pipeline with the DecisionTreeClassifier model so we were more focused on finding the best pair of hyperparameters than on finding a different model. 

Our final model, fitted on the same training set as the baseline model and evaluated on the same testing set had a mean accuracy of around 94.3%. That's a huge improvement from our baseline model which had a mean accuracy of around 52.1%. This is likely do to the fact that we used more features and better features. If we checked the percentage of how many of each position was correctly guessed, we could see a noticeable difference. Here's is how our Final Model performed on the testing dataset. 
| position   |        0 |
|:-----------|---------:|
| bot        | 0.944533 |
| jng        | 0.995524 |
| mid        | 0.891535 |
| sup        | 0.970116 |
| top        | 0.913216 |
## Fairness Analysis
In order to perform a “fairness analysis” of our final model from the previous step, we decided to compare our model’s predictions on the type of ‘league’ to see if there was a significant difference in mean accuracy between the two leagues that we chose. The two leagues that we decided to compare were the ‘LPL’ and ‘LCK CL’. The question that we want to answer through doing this process is: does our model perform worse for individuals in one group (LCK CL or LPL) than it does for individuals in the other group (LCK CL or LPL)? 
To compare our model’s fairness between these two groups, we decided to compare their accuracies. Since we want to compare the difference between two samples in distribution, we used a permutation test to figure this out. 
The null hypothesis for our test is: Our model is fair, Its accuracy for predicting ‘position’ for groups in the ‘LCK CL’ and the ‘LPL’ are the same.
The alternative hypothesis for our test is: Our model is not fair, Its accuracy for predicting ‘position’ for groups in the ‘LCK CL’ and the ‘LPL’ are not the same.


